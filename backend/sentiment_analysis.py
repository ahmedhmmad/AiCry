# sentiment_analysis.py - Ù†Ø³Ø®Ø© ØªØ³ØªØ®Ø¯Ù… API Keys Ø­Ù‚ÙŠÙ‚ÙŠØ©
import os
import logging
import time
from typing import Dict, Any, List
from datetime import datetime, timedelta
import json
import asyncio
from dotenv import load_dotenv

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
load_dotenv()

# Ø¥Ø¹Ø¯Ø§Ø¯ logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RealSentimentAnalyzer:
    def __init__(self):
        self.twitter_client = None
        self.reddit_client = None
        self.news_client = None
        self.telegram_client = None
        self.vader_analyzer = None

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self.cache = {}
        self.cache_duration = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚

        # ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        self._initialize_services()

    def _initialize_services(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø®Ø¯Ù…Ø§Øª API"""
        logger.info("ğŸš€ Initializing real sentiment analysis services...")

        # ØªÙ‡ÙŠØ¦Ø© VaderSentiment
        self._init_vader()

        # ØªÙ‡ÙŠØ¦Ø© Twitter API
        self._init_twitter()

        # ØªÙ‡ÙŠØ¦Ø© Reddit API
        self._init_reddit()

        # ØªÙ‡ÙŠØ¦Ø© News API
        self._init_news()

        # ØªÙ‡ÙŠØ¦Ø© Telegram Bot
        self._init_telegram()

    def _init_vader(self):
        """ØªÙ‡ÙŠØ¦Ø© VaderSentiment"""
        try:
            from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
            self.vader_analyzer = SentimentIntensityAnalyzer()
            logger.info("âœ… VaderSentiment initialized successfully")
        except ImportError:
            logger.warning("âš ï¸ VaderSentiment not available")
            self.vader_analyzer = None

    def _init_twitter(self):
        """ØªÙ‡ÙŠØ¦Ø© Twitter API"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØ§ØªÙŠØ­ Twitter Ù…Ù† .env
            api_key = os.getenv('TWITTER_API_KEY')
            api_secret = os.getenv('TWITTER_API_SECRET')
            access_token = os.getenv('TWITTER_ACCESS_TOKEN')
            access_token_secret = os.getenv('TWITTER_ACCESS_TOKEN_SECRET')
            bearer_token = os.getenv('TWITTER_BEARER_TOKEN')

            if all([api_key, api_secret, access_token, access_token_secret]):
                import tweepy

                # ØªÙ‡ÙŠØ¦Ø© Twitter API v2
                self.twitter_client = tweepy.Client(
                    bearer_token=bearer_token,
                    consumer_key=api_key,
                    consumer_secret=api_secret,
                    access_token=access_token,
                    access_token_secret=access_token_secret,
                    wait_on_rate_limit=True
                )

                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                me = self.twitter_client.get_me()
                logger.info(f"âœ… Twitter API initialized successfully - Connected as: {me.data.username}")

            else:
                logger.warning("âš ï¸ Twitter API keys not found in .env file")
                self.twitter_client = None

        except Exception as e:
            logger.error(f"âŒ Twitter API initialization failed: {e}")
            self.twitter_client = None

    def _init_reddit(self):
        """ØªÙ‡ÙŠØ¦Ø© Reddit API"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØ§ØªÙŠØ­ Reddit Ù…Ù† .env
            client_id = os.getenv('REDDIT_CLIENT_ID')
            client_secret = os.getenv('REDDIT_CLIENT_SECRET')
            user_agent = os.getenv('REDDIT_USER_AGENT', 'CryptoSentimentBot/1.0')

            if all([client_id, client_secret]):
                import praw

                self.reddit_client = praw.Reddit(
                    client_id=client_id,
                    client_secret=client_secret,
                    user_agent=user_agent
                )

                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                test_sub = self.reddit_client.subreddit('cryptocurrency')
                logger.info(f"âœ… Reddit API initialized successfully - Test subreddit: {test_sub.display_name}")

            else:
                logger.warning("âš ï¸ Reddit API keys not found in .env file")
                self.reddit_client = None

        except Exception as e:
            logger.error(f"âŒ Reddit API initialization failed: {e}")
            self.reddit_client = None

    def _init_news(self):
        """ØªÙ‡ÙŠØ¦Ø© News API"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØªØ§Ø­ News API Ù…Ù† .env
            api_key = os.getenv('NEWS_API_KEY')

            if api_key:
                from newsapi import NewsApiClient

                self.news_client = NewsApiClient(api_key=api_key)

                # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
                test_sources = self.news_client.get_sources()
                logger.info(f"âœ… News API initialized successfully - Available sources: {len(test_sources['sources'])}")

            else:
                logger.warning("âš ï¸ News API key not found in .env file")
                self.news_client = None

        except Exception as e:
            logger.error(f"âŒ News API initialization failed: {e}")
            self.news_client = None

    def _init_telegram(self):
        """ØªÙ‡ÙŠØ¦Ø© Telegram Bot"""
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…ÙØªØ§Ø­ Telegram Bot Ù…Ù† .env
            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')

            if bot_token:
                from telegram import Bot

                self.telegram_client = Bot(token=bot_token)
                logger.info("âœ… Telegram Bot initialized successfully")

            else:
                logger.warning("âš ï¸ Telegram Bot token not found in .env file")
                self.telegram_client = None

        except Exception as e:
            logger.error(f"âŒ Telegram Bot initialization failed: {e}")
            self.telegram_client = None

    def _is_cache_valid(self, cache_key: str) -> bool:
        """ÙØ­Øµ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        if cache_key in self.cache:
            data, timestamp = self.cache[cache_key]
            return time.time() - timestamp < self.cache_duration
        return False

    def _get_cached_data(self, cache_key: str):
        """Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        if cache_key in self.cache:
            return self.cache[cache_key][0]
        return None

    def _cache_data(self, cache_key: str, data: Any):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        self.cache[cache_key] = (data, time.time())

    async def fetch_twitter_sentiment(self, symbol: str) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Twitter"""
        if not self.twitter_client:
            return {"error": "Twitter API not available"}

        try:
            # Ø¥Ø²Ø§Ù„Ø© USDT Ù…Ù† Ø§Ù„Ø±Ù…Ø² Ù„Ù„Ø¨Ø­Ø«
            crypto_name = symbol.replace('USDT', '').replace('BUSD', '')

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª
            query = f"({crypto_name} OR ${crypto_name}) -is:retweet lang:en"

            tweets = tweepy.Paginator(
                self.twitter_client.search_recent_tweets,
                query=query,
                max_results=100,
                tweet_fields=['created_at', 'public_metrics']
            ).flatten(limit=100)

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            sentiments = []
            total_engagement = 0

            for tweet in tweets:
                if tweet.text and self.vader_analyzer:
                    sentiment_scores = self.vader_analyzer.polarity_scores(tweet.text)
                    sentiments.append(sentiment_scores['compound'])

                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„
                    if hasattr(tweet, 'public_metrics'):
                        engagement = (
                                tweet.public_metrics.get('like_count', 0) +
                                tweet.public_metrics.get('retweet_count', 0) +
                                tweet.public_metrics.get('reply_count', 0)
                        )
                        total_engagement += engagement

            if not sentiments:
                return {"error": "No tweets found"}

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            avg_sentiment = sum(sentiments) / len(sentiments)
            sentiment_score = round((avg_sentiment + 1) * 50, 1)  # ØªØ­ÙˆÙŠÙ„ Ù…Ù† -1,1 Ø¥Ù„Ù‰ 0,100

            return {
                "source": "twitter",
                "sentiment_score": sentiment_score,
                "trend": "bullish" if avg_sentiment > 0.1 else "bearish" if avg_sentiment < -0.1 else "neutral",
                "posts_analyzed": len(sentiments),
                "total_engagement": total_engagement,
                "confidence": min(len(sentiments) * 2, 95),  # Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª = Ø«Ù‚Ø© Ø£Ø¹Ù„Ù‰
                "raw_sentiment": avg_sentiment
            }

        except Exception as e:
            logger.error(f"âŒ Twitter sentiment analysis failed: {e}")
            return {"error": f"Twitter analysis failed: {str(e)}"}

    async def fetch_reddit_sentiment(self, symbol: str) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Reddit"""
        if not self.reddit_client:
            return {"error": "Reddit API not available"}

        try:
            crypto_name = symbol.replace('USDT', '').replace('BUSD', '')

            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ subreddits Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
            subreddits = ['cryptocurrency', 'CryptoMarkets', 'Bitcoin', 'ethereum', 'altcoin']

            all_posts = []
            for subreddit_name in subreddits:
                try:
                    subreddit = self.reddit_client.subreddit(subreddit_name)

                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø­Ø¯ÙŠØ«Ø©
                    for submission in subreddit.search(crypto_name, time_filter='week', limit=20):
                        if submission.title and submission.selftext:
                            text = f"{submission.title} {submission.selftext}"
                            all_posts.append({
                                'text': text,
                                'score': submission.score,
                                'comments': submission.num_comments,
                                'created': submission.created_utc
                            })

                except Exception as e:
                    logger.warning(f"Error accessing subreddit {subreddit_name}: {e}")
                    continue

            if not all_posts:
                return {"error": "No Reddit posts found"}

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            sentiments = []
            total_score = 0
            total_comments = 0

            for post in all_posts:
                if self.vader_analyzer:
                    sentiment_scores = self.vader_analyzer.polarity_scores(post['text'])
                    sentiments.append(sentiment_scores['compound'])
                    total_score += post['score']
                    total_comments += post['comments']

            if not sentiments:
                return {"error": "No sentiment data"}

            avg_sentiment = sum(sentiments) / len(sentiments)
            sentiment_score = round((avg_sentiment + 1) * 50, 1)

            return {
                "source": "reddit",
                "sentiment_score": sentiment_score,
                "trend": "bullish" if avg_sentiment > 0.1 else "bearish" if avg_sentiment < -0.1 else "neutral",
                "posts_analyzed": len(sentiments),
                "total_score": total_score,
                "total_comments": total_comments,
                "confidence": min(len(sentiments) * 3, 95),
                "raw_sentiment": avg_sentiment
            }

        except Exception as e:
            logger.error(f"âŒ Reddit sentiment analysis failed: {e}")
            return {"error": f"Reddit analysis failed: {str(e)}"}

    async def fetch_news_sentiment(self, symbol: str) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"""
        if not self.news_client:
            return {"error": "News API not available"}

        try:
            crypto_name = symbol.replace('USDT', '').replace('BUSD', '')

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            news = self.news_client.get_everything(
                q=f'"{crypto_name}" OR "cryptocurrency"',
                language='en',
                sort_by='publishedAt',
                from_param=(datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'),
                page_size=50
            )

            articles = news.get('articles', [])

            if not articles:
                return {"error": "No news articles found"}

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
            sentiments = []

            for article in articles:
                title = article.get('title', '')
                description = article.get('description', '')

                if title and self.vader_analyzer:
                    text = f"{title} {description}" if description else title
                    sentiment_scores = self.vader_analyzer.polarity_scores(text)
                    sentiments.append(sentiment_scores['compound'])

            if not sentiments:
                return {"error": "No sentiment data from news"}

            avg_sentiment = sum(sentiments) / len(sentiments)
            sentiment_score = round((avg_sentiment + 1) * 50, 1)

            return {
                "source": "news",
                "sentiment_score": sentiment_score,
                "trend": "bullish" if avg_sentiment > 0.1 else "bearish" if avg_sentiment < -0.1 else "neutral",
                "articles_analyzed": len(sentiments),
                "confidence": min(len(sentiments) * 4, 95),
                "raw_sentiment": avg_sentiment
            }

        except Exception as e:
            logger.error(f"âŒ News sentiment analysis failed: {e}")
            return {"error": f"News analysis failed: {str(e)}"}

    async def get_comprehensive_sentiment(self, symbol: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
        cache_key = f"comprehensive_{symbol}"

        # ÙØ­Øµ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        if self._is_cache_valid(cache_key):
            logger.info(f"âœ… Using cached comprehensive sentiment for {symbol}")
            return self._get_cached_data(cache_key)

        logger.info(f"ğŸ”„ Fetching real sentiment data for {symbol}")

        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
        tasks = []

        if self.twitter_client:
            tasks.append(self.fetch_twitter_sentiment(symbol))

        if self.reddit_client:
            tasks.append(self.fetch_reddit_sentiment(symbol))

        if self.news_client:
            tasks.append(self.fetch_news_sentiment(symbol))

        # ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù…
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        except Exception as e:
            logger.error(f"âŒ Error in comprehensive sentiment analysis: {e}")
            results = []

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        valid_results = []
        sources_data = {}

        for result in results:
            if isinstance(result, dict) and "error" not in result:
                valid_results.append(result)
                source_name = result.get("source", "unknown")
                sources_data[source_name] = result

        if not valid_results:
            # Ø¥Ø±Ø¬Ø§Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ø¹ ØªØ­Ø°ÙŠØ±
            return self._generate_fallback_data(symbol, "No real data available from any source")

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
        total_sentiment = 0
        total_weight = 0

        weights = {
            "twitter": 0.4,  # ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰ Ù„ØªÙˆÙŠØªØ±
            "reddit": 0.35,  # ÙˆØ²Ù† Ù…ØªÙˆØ³Ø· Ù„Ø±ÙŠØ¯ÙŠØª
            "news": 0.25  # ÙˆØ²Ù† Ø£Ù‚Ù„ Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
        }

        for result in valid_results:
            source = result.get("source", "unknown")
            sentiment = result.get("sentiment_score", 50)
            confidence = result.get("confidence", 50)

            weight = weights.get(source, 0.1) * (confidence / 100)
            total_sentiment += sentiment * weight
            total_weight += weight

        if total_weight == 0:
            return self._generate_fallback_data(symbol, "No weighted sentiment calculated")

        final_score = total_sentiment / total_weight

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        if final_score >= 60:
            trend = "bullish"
        elif final_score <= 40:
            trend = "bearish"
        else:
            trend = "neutral"

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        result = {
            "symbol": symbol,
            "overall_score": round(final_score, 1),
            "trend": trend,
            "confidence": round(min(total_weight * 100, 95), 1),
            "sources": sources_data,
            "analysis_type": "real_data_comprehensive",
            "timestamp": datetime.now().isoformat(),
            "cache_duration": self.cache_duration,
            "next_update": (datetime.now() + timedelta(seconds=self.cache_duration)).strftime("%H:%M:%S"),
            "sources_available": len(valid_results),
            "total_posts_analyzed": sum(r.get("posts_analyzed", 0) for r in valid_results)
        }

        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self._cache_data(cache_key, result)

        logger.info(f"âœ… Generated real comprehensive sentiment for {symbol}: {final_score:.1f} ({trend})")
        return result

    def _generate_fallback_data(self, symbol: str, reason: str) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ø¹ Ø³Ø¨Ø¨ Ø§Ù„ÙØ´Ù„"""
        return {
            "symbol": symbol,
            "overall_score": 50,
            "trend": "neutral",
            "confidence": 30,
            "sources": {},
            "analysis_type": "fallback",
            "timestamp": datetime.now().isoformat(),
            "error_reason": reason,
            "note": "Real sentiment analysis failed - using fallback data"
        }

    # Ø¯ÙˆØ§Ù„ Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
    def get_quick_sentiment(self, symbol: str) -> Dict[str, Any]:
        """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹"""
        try:
            # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self.get_comprehensive_sentiment(symbol))
            loop.close()

            return {
                "summary": f"ğŸ“Š {symbol}: {result.get('trend', 'neutral')} - {result.get('overall_score', 50)}",
                "symbol": symbol,
                "sentiment_score": result.get('overall_score', 50),
                "trend": result.get('trend', 'neutral'),
                "confidence": result.get('confidence', 50),
                "source": result.get('analysis_type', 'real_data'),
                "timestamp": result.get('timestamp', datetime.now().isoformat())
            }

        except Exception as e:
            logger.error(f"âŒ Quick sentiment failed: {e}")
            return self._generate_fallback_data(symbol, str(e))

    def get_complete_analysis(self, symbol: str) -> Dict[str, Any]:
        """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self.get_comprehensive_sentiment(symbol))
            loop.close()
            return result
        except Exception as e:
            logger.error(f"âŒ Complete analysis failed: {e}")
            return self._generate_fallback_data(symbol, str(e))

    def get_enhanced_analysis(self, symbol: str) -> Dict[str, Any]:
        """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†"""
        complete = self.get_complete_analysis(symbol)

        # Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ù…Ø­Ø³Ù†Ø©
        if "error_reason" not in complete:
            # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
            sources = complete.get('sources', {})

            enhanced_features = {
                "market_psychology": self._analyze_market_psychology(sources),
                "sentiment_momentum": self._calculate_momentum(sources),
                "fear_greed_index": self._calculate_fear_greed(complete.get('overall_score', 50))
            }

            complete["enhanced_features"] = enhanced_features
            complete["analysis_type"] = "real_data_enhanced"

        return complete

    def _analyze_market_psychology(self, sources: Dict) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ù„Ù… Ø§Ù„Ù†ÙØ³ Ø§Ù„Ø³ÙˆÙ‚ÙŠ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
        twitter_data = sources.get('twitter', {})
        reddit_data = sources.get('reddit', {})

        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹ Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„Ù†Ù‚Ø§Ø·
        twitter_engagement = twitter_data.get('total_engagement', 0)
        reddit_score = reddit_data.get('total_score', 0)

        greed_level = min((twitter_engagement / 1000) * 20 + (reddit_score / 100) * 10, 100)
        fear_level = 100 - greed_level

        return {
            "greed_level": round(greed_level, 1),
            "fear_level": round(fear_level, 1),
            "market_activity": "high" if twitter_engagement > 500 else "medium" if twitter_engagement > 100 else "low"
        }

    def _calculate_momentum(self, sources: Dict) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ø²Ø®Ù… Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        total_posts = sum(source.get('posts_analyzed', 0) for source in sources.values())
        avg_sentiment = sum(source.get('raw_sentiment', 0) for source in sources.values()) / max(len(sources), 1)

        return {
            "momentum_score": round(avg_sentiment * 50, 1),
            "trend_direction": "improving" if avg_sentiment > 0.1 else "declining" if avg_sentiment < -0.1 else "stable",
            "data_volume": total_posts
        }

    def _calculate_fear_greed(self, overall_score: float) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø®ÙˆÙ ÙˆØ§Ù„Ø¬Ø´Ø¹"""
        if overall_score >= 75:
            level = "Extreme Greed"
        elif overall_score >= 60:
            level = "Greed"
        elif overall_score >= 45:
            level = "Neutral"
        elif overall_score >= 30:
            level = "Fear"
        else:
            level = "Extreme Fear"

        return {
            "index_value": round(overall_score, 1),
            "level": level,
            "interpretation": f"Market showing {level.lower()} based on real sentiment data"
        }


# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
sentiment_analyzer = RealSentimentAnalyzer()